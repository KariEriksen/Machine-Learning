\documentclass[a4paper,12pt, english]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{babel}
\usepackage{amsmath}
\usepackage{ulem}
\usepackage{a4wide}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{tabularx}
\usepackage{tabulary}
\usepackage{hyperref}

\begin{document}

\begin{titlepage}
\begin{center}
\textsc{\Large Machine Learning}\\[0.2cm]
\textsc{FYS-STK 4155}\\[1.0cm]
\textsc{\Large Project 2}\\[0.2cm]
\textsc{Kari Eriksen}\\[1.0cm]

\begin{abstract}

\end{abstract}

\end{center}
\end{titlepage}

\tableofcontents

\newpage

\section{Introduction}

In this project we will deal with different methods within the field of machine learning to solve different types of problems, such as classification and regression. We will be looking at the Ising model in both 1D and 2D. We begin with estimating the coupling constant $J$ for the 1 dimensional case using linear regression methods and evaluating the calculations with bootstrap. In 2 dimension the Ising model experiances a phase transition when there is a change in temperature. This phase shift we can define as a classification problem and try to train the different models to identify these. We will be using both logistic regression and a neural network for this purpose. 



\section{Theory}

\subsection{Ising model}

The Ising model is a mathematical model named after Ernst Ising and it describes a system of spins in a lattice where the energy of the system can be found through eq. \ref{eq:Ising}. J represents the coupling constant, $s_k$ and $s_l$ are spins with value either +1 or -1 (up or don), N is the total number of spins in the system and B is the external magnetic field which in this project is zero. We therefore look at a system with energy equal to eq. \ref{eq:Ising_model}. 
The spins themself represents magnetic dipole moments and the lattice allow each spin to interact with its neighbors, indicated by the symbol $<kl>$ in the sum. 

\begin{equation} \label{eq:Ising}
E = -J \sum_{<kl>}^N s_k s_l - B \sum_k^N s_k
\end{equation}

\begin{equation} \label{eq:Ising_model}
E = -J \sum_{<kl>}^N s_k s_l
\end{equation}

In 1 dimension there is no phase transition and we will use regression in order to determine the coupling constant between the spins in the system.  \\
In 2 dimensions or higher however the system goes through a phase transition with change in temperature. Below the critical temperature, $T_C \approx 2.269$, the system is what we will call an ordered state. The spins tend to be aligned which causes a net magnetization of the system, also described as a ferromagnet. Above the critical temperature the coupling constant is smaller than zero and the spins interact in an antiferromagnetic way. We wish to train our model to classify a configurations of spins as an ordered or disordered system. \\
To do so we begin with the 1D Ising model with nearest-neighbor interactions, eq. \ref{eq:ising_1D}. 

\begin{equation} \label{eq:ising_1D}
E[s] = -J \sum_{j=1}^L s_j s_{j+1}
\end{equation}

Now we have $L$ number of spins in a one dimensional array. If we where to have a data set $i = 1, ..., n$ of different configurations on the form $\{(E[s^i], s^i)\}$ we could use this in training a linear model to find the coupling constant. That way we could use much of the code from project 1. To do so we use the all-to-all Ising model and notice that the energy is linear in $J$.  

\begin{equation}
E_{model}[s^i] = -\sum_{j=1}^L \sum_{k=1}^L J_{j,k} s_j^i s_k^i
\end{equation}

We can now recast this problem to a linear regression model, rewriting all two-body interactions $\{s_j^i s_k^i\}_{j,k}^L$ as a vector $\mathbf{X}^i$. 

\begin{equation}
E_{model}^i \equiv \mathbf{X}^i \cdot \mathbf{J}
\end{equation}

\section{Methods}

\subsection{Linear Regression}

Linear regression is a method in statistics that predicts the response of one or several explanatory variables. It assumes a linear relationship between the dependent and independent variables. At its simplest form we could try to find the stright line between two points. This equation is fairly simple.

\begin{equation*}
\hat{y} = \hat{\beta_{0}} + \hat{\beta_{1}}x + \hat{\epsilon}
\end{equation*}

Here $\hat{y}$ is a dependent variable, the outcome, $x$ is an independent variable, or the predictor, and $\hat{\beta_{0}}$ and $\hat{\beta_{1}}$ the intercept and slope respectively. $\epsilon$ is the error in our prediction. The solution for $\hat{\beta_{0}}$ and $\hat{\beta_{1}}$ in this problem is best found with least square and is also fairly easy. Calculating the mean over both variables $(\bar{x}$ and $\bar{y})$ we can find the parameters that give the prediction that differs the least from the exact solution.

\begin{equation*}
\beta_1 = \frac{\sum^n (x_i - \bar{x})(y_i - \bar{y})}{\sum^n (x_i - \bar{x})^2}
\end{equation*}

\begin{equation*}
\beta_0 = \bar{y} - \beta_1 \bar{x}
\end{equation*}

If we have several predictors we can extend our problem to a more general case.

\begin{equation*}
\hat{y} = f(X) = \beta_{0} + \sum_{j=1}^{p} X_{j} \beta_{j}
\end{equation*}

Now $X$ is a vector containing all predictors, $X^{\top} = \{X_0, X_1, X_2, X_3,..., X_p\}$, $\beta_0$ is the intercept and $\beta_j$ is a vector keeping all coefficients for each predictor, the parameters we are searching for. $\hat{y}$ is the predicted values of $y = f(X)$. Moving $\beta_0$ to the $\beta-\textnormal{vector}$ and adding an extra column with 1's to the design matrix $X^{\top}$ we can reduce the problem to vector form and get the following. We will make use of this notation when finding solutions using least square etc. 

\begin{equation} \label{eq:y_predict}
\hat{y} = \hat{X} \hat{\beta} + \epsilon
\end{equation}


\subsubsection{Ordinary Least Square} \label{sec:OLS}

The least square method selects the parameters $\beta$ so that residual sum of squares (RSS) is minimized.

\begin{equation*}
\textnormal{RSS}(\beta) = \sum_{i=1}^{N} (y_i - x_i^{\top}\beta)^2
\end{equation*}

$y_i$ is still the independent variable, and $x_i^{\top}\beta$ represents the prediction of outcome given the calculated parameter $\beta$. And the difference between these variables squared gives us the RSS of the parameter $\beta$. $\beta$ is a vector $p + 1$ long, the number of features (plus the intercept) in the design matrix. 

This can be expressed in matrix notation, using eq. \ref{eq:y_predict}. To find an expression for the $\beta$-parameter we look for the minimum of the RSS, meaning we take its derivative wrt. $\beta$.

\begin{equation*}
\textnormal{RSS}(\beta) = (\mathbf{y} - \mathbf{X}\beta)^{\top}(\mathbf{y} - \mathbf{X}\beta)
\end{equation*}

\begin{equation*}
\frac{\partial \textnormal{RSS}(\beta)}{\partial \beta} = 0 = -2 \mathbf{X}^{\top}(\mathbf{y} - \mathbf{X}\beta)
\end{equation*}

\begin{equation*}
\mathbf{X}^{\top}\mathbf{y} - \mathbf{X}^{\top}\mathbf{X} \beta = 0
\end{equation*}

\begin{equation} \label{eq:beta}
\hat{\beta} = (\mathbf{X}^{\top}\mathbf{X})^{-1}\mathbf{X}^{\top}\mathbf{y}
\end{equation}

This is the expresion we use in the ordinary least square-method in order to find the optimal $\beta$-values. This method depends on the propertie $\mathbf{X^{\top}}\mathbf{X}$ being postive definit in order to be able to calculate its inverse. In case it is not we must use other method.

\subsubsection{Ridge Regression}

As mentioned in the section above we may come across problems where the columns in $X$ are not linear independent, often an issue for problems in high dimesions. Then the coefficients in $\beta$ are not uniquely defined through least square. This was the motivation for what would be the Ridge regression, an ad hoc solution to the singularity of $\mathbf{X^{\top}}\mathbf{X}$ introduced by Hoerl and Kennard (1970).
They suggested adding a tuning parameter $\lambda$, i.e. a penalty to the sizes of the coefficients.

\begin{equation*}
\mathbf{X}^{\top}\mathbf{X} \ \rightarrow \ \mathbf{X}^{\top}\mathbf{X} + \lambda\mathbf{I}
\end{equation*}

By doing the replacement above we are able to calculate the inverse and can again find the expression for $\beta$ but this time through minimizing the penalized RSS. The solution for $\beta$ is eq. \ref{eq:beta}, which is now dependent on the parameter $\lambda$.

\begin{equation*}
\textnormal{PRSS}(\beta) = (\mathbf{y} - \mathbf{X}\beta)^{\top}(\mathbf{y} - \mathbf{X}\beta) + \lambda ||\beta||^2
\end{equation*}

\begin{equation*}
\frac{\partial \textnormal{PRSS}(\beta)}{\partial \beta} = 0 = -2 \mathbf{X}^{\top}(\mathbf{y} - \mathbf{X}\beta) + 2 \lambda \beta
\end{equation*}

\begin{equation*} 
\hat{\beta}(\lambda) = (\mathbf{X}^{\top}\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}^{\top}\mathbf{y}
\end{equation*}

$\mathbf{I}$ is the identity matrix, a $p \times p$ matrix, and $\lambda \in [0, \infty]$. The tuning parameter $\lambda$ determines the regularization of the problem and different $\lambda$ will give different solution to the regression problem. Our task will be to find an optimal parameter for our case.

\begin{equation} \label{eq:b_ridge}
\hat{\beta}^{ridge} = \textnormal{argmin}_{\beta} \left \{ \sum_{i=1}^{N} (y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right \}
\end{equation}

We can see from eq. \ref{eq:b_ridge} that the method assumes our design matrix is centered, the intercept does not depend on the tuning parameter. $\beta_0$ is instead found by calculating the mean of $y$.

\begin{equation} \label{y_mean}
\beta_0 = \bar{y} = \frac{1}{N} \sum_i^N y_i
\end{equation}

\subsubsection{Lasso Regression}

In 1996 Tibshirani suggested a new penalty, the Lasso. Similar to ridge regression but the difference lies in the last part.  

\begin{equation*}
\hat{\beta}^{lasso} = \textnormal{argmin}_{\beta} \left \{ \sum_{i=1}^{N} (y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right \}
\end{equation*}

As for the ridge regression the intercept is given by the mean of $y$.

\subsection{Singular value decomposition}

As mentioned in \ref{sec:OLS} we are dealing with a design matrix that is singular meaning $\mathbf{X}^{\top}\mathbf{X}$ is not invertible. One way to solve this problem is with the use of singular value decomposition (SVD). We can rewrite $\mathbf{X}$ as the factorization of a $n \times p$ unitary matrix $\mathbf{U}$, a $p \times p$ diagonal matrix $\mathbf{\Sigma}$ and the conjugate transpose of a $p \times p$ unitary matrix $\mathbf{V}$.

\begin{equation*} 
\mathbf{X} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\top
\end{equation*}

Now the inverse of the matrix product $\mathbf{X}^{\top}\mathbf{X}$ can be written as the inverse of the matrix product of the SVD.

The Moore-Penrose pseudoinverse is defined as $\mathbf{X}^+ = (\mathbf{X}^{\top}\mathbf{X})^{-1}$. Using the SVD we get eq. \ref{pseudo}.

\begin{equation} \label{pseudo}
\mathbf{X}^+ = \mathbf{V} \mathbf{\Sigma}^+ \mathbf{U}^\top 
\end{equation}

Now we can rewrite the solution of $\beta$ in eq. \ref{eq:beta} to the following.

\begin{equation*}
\beta = \mathbf{V} \mathbf{\Sigma}^+ \mathbf{U}^\top \mathbf{y}
\end{equation*}

\subsection{Logistic regression}

We now move on to the classification part of the project and look closer on logistic regression. Where regression methods focus on predicting continuous variables of a response a classification method look at outcomes in form of discrete variables (i.e. categories).  \\
Our aim is to make a classifier that can take in a number of spin-state congigurations and predict which categories these configurations belong to, the mentioned ordered and disordered phase.

A simple classifier that is easy to understand and has some similarities to the regression methods above is a linear classifier that categorizes according to sign function. So for values above a certain threshold it maps the output to a category and for values below it maps it to another. But the function itself takes in continuous data, eq. \ref{eq:log_data}.

\begin{equation} \label{eq:log_data}
\mathbf{x}_i^{\top} \mathbf{w} + b_0 \equiv \mathbf{x}_i^{\top} \mathbf{w} 
\end{equation}

But instead of a sign function we could, given $\mathbf{x}_i$, calculate the probability of the outcome being in a certain category. To calculate this probability we use the sigmoid function, eq. \ref{eq:sigmoid}, and the outsome is given as $y_i = \{0,1\}$.

\begin{equation} \label{eq:sigmoid}
f(\mathbf{x}_i^{\top} \mathbf{w}) = \frac{1}{1 + e^{-\mathbf{x}_i^{\top} \mathbf{w}}}
\end{equation} 

\begin{equation*}
P(y_i = 1|\mathbf{x}_i,\mathbf{\theta}) = \frac{1}{1 + e^{-\mathbf{x}_i^{\top} \mathbf{w}}}
\end{equation*}

Since we are only dealing with a binary problem, the phase can either be ordered ($y_i = 1$), or disordered ($y_i = 0$) and we have that 

\begin{equation*}
P(y_i = 0|\mathbf{x}_i,\mathbf{\theta}) = 1 - P(y_i = 1|\mathbf{x}_i,\mathbf{\theta}).
\end{equation*}

To find the cost function of the logistic regression we we use the Maximum Likelihood Estimation (MLE). We want to maximize the probability of a configuration being in a category. This gives us the cross entropy \ref{cross_entropy}.

\begin{equation} \label{cross_entropy}
C(\beta) = \sum_{i=1}^N -y_i \log (f(X_i^\top \beta) - (1 - y_i) \log [1 - f(X_i^\top \beta)])
\end{equation}

\subsection{Neural Network}

\begin{align}
\Delta_j^l &= \frac{\partial E}{\partial z_j^l} = \frac{\partial E}{\partial a_j^l} \sigma'(z_j^l) \\
\Delta_j^l &= \frac{\partial E}{\partial z_j^l} = \frac{\partial E}{\partial b_j^l} \frac{\partial b_j^l}{\partial z_j^l} = \frac{\partial E}{\partial b_j^l} \\
\Delta_j^l &= \frac{\partial E}{\partial z_j^l} = \sum_k \frac{\partial E}{\partial z_k^{l+1}} \frac{\partial z_j^{l+1}}{\partial z_j^l} \\
&= \sum_k \Delta_k^{l+1} \frac{\partial z_k^{l+1}}{z_k^l} \\
&= \left( \sum_k \Delta_k^{l+1} \beta_{kj}^{l+1} \right) \sigma'(z_j^l) \\
\frac{\partial E}{\partial \beta_{jk}^l} &= \frac{\partial E}{\partial z_j^l} \frac{\partial z_j^l}{\partial \beta_{jk}^l} = \Delta_j^l a_k^{l+1} 
\end{align}



\subsection{Stochastic gradient descent}

Gradient descent 

minimizing the cost function

\begin{equation}
E(\theta) = \sum_{i=1}^N e_i(X_i, \theta) 
\end{equation}

\begin{equation}
\mathbf{v_t} = \eta_t \nabla_{\theta} E(\theta_t)
\end{equation}

\begin{equation}
\theta_{t+1} = \mathbf{\theta_t} - \mathbf{v_t}
\end{equation}

\begin{equation}
\nabla_{\theta} E(\theta) = \sum_i^n \nabla_{\theta} e_i(\mathbf{x}_i, \mathbf{\theta}) \longrightarrow \sum_{i \in B_k} \nabla_{\theta} e_i(\mathbf{x}_i, \mathbf{\theta})
\end{equation}

\begin{equation}
\nabla_{\theta} E^{MB} (\theta) = \sum_{i \in B_k}^M \nabla_{\theta} e_i(\mathbf{x}_i, \mathbf{\theta})
\end{equation}

\begin{equation}
\mathbf{v_t} = \eta_t \nabla_{\theta} E^{MB} (\theta)
\end{equation}

\begin{equation}
\theta_{t+1} = \mathbf{\theta_t} - \mathbf{v_t}
\end{equation}

\section{Resampling Methods}

\subsection{Bootstrap}

The bootstrap is a resampling method suggested by Efron in 1979. It tell us how well our regression models assess the problem at hand. To say something about this it is commen to calculate the statistical properties given in the section above, particularly the MSE. As we can see from eq. \ref{eq:MSE} this is a measurment consisting of three properties. The variance, the bias and the irreducible error.	\\
In machine learning phenomena such as overfitting and underfitting are highly important to be aware of and is connected with the MSE. Our goal is to predict the outcome $\hat{y}$ given some observed data. To do so we split our data into a training set and a test set and train the model with the training data. Depending on how well we fit our model to the training data we may overfit or underfit. Overfitting means that we fit the data so well that we have made the model to close and dependent on the training data. 
On the other hand we can underfit, meaning that we miss many important features of the data. In both cases trying the model out on the test set we get bad results. This is what is known as the bias-variance tradeoff. \\
The variance tells us how the predicted outcome differs from its mean, and a high variance will correspond to overfitting. The bias says how much difference there is between the models predicted value and the true value. A high bias corresponds to underfitting. \\
Since our predictor is a random variable, how we draw the data will effect the estimate of the response. In bootstrap we draw samples with replacement from our dataset (the training data) and fit the model with this. Then we test the model with the test data and calculate the errors. Doing this many times we can examine the behavoir of the fit and get a more accurate estimate of the errors.

\section{Statistical Properties}

\begin{align} \label{eq:MSE}
\textnormal{MSE} &= \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y_i})^2 \\
&= \frac{1}{N} \sum_{i=1}^N (\hat{y_i} - \bar{\hat{y_i}})^2 + \frac{1}{N} \sum_{i=1}^N (y_i - \bar{\hat{y_i}})^2 + \frac{1}{N} \sum_{i=1}^N (y_i - \bar{\hat{y_i}})(\bar{\hat{y_i}} - \hat{y}) \\
&= Var(\hat{y}) + Bias + \epsilon
\end{align}

\begin{equation}
\textnormal{R}^2(y,\hat{y}) = 1 - \frac{\sum_{n}^{i=1} (y_i - \hat{y_i})^2}{\sum_{n}^{i=1}(y_i - \bar{y})^2}
\end{equation}

\section{Implementation}

\subsection{Part b: 1D Ising with linear regression}

\subsection{Part c: 2D Ising with logistic regression}

\subsection{Part d: 1D Ising with neural network}

\subsection{Part e: 2D Ising with neural network}


\section{Results}

\subsection{Part b: 1D Ising with linear regression}

\subsection{Part c: 2D Ising with logistic regression}

\subsection{Part d: 1D Ising with neural network}

\subsection{Part e: 2D Ising with neural network}

\subsection{Part f: Critical evaluation}

\section{Discussion}

\section{Conclusion}

\newpage

\begin{thebibliography}{9}

\bibitem{elem}
  Trevor Hastie,
  \textit{The Elements of Statistical Learning},
  Springer, New York,
  2nd edition,
  2009.

\bibitem{IntroStat}
  Gareth James, 
  \textit{An Introduction to Statistical Learning},
  Springer, New York,
  2013.
  
\bibitem{morten}
  \url{https://compphysics.github.io/MachineLearning/doc/pub/Regression/html/._Regression-bs000.html}, 08/10-18.
  
\bibitem{berk}
  \url{https://ml.berkeley.edu/blog/2017/07/13/tutorial-4/}, 08/10-18.
  
\bibitem{high-bias}
  \url{https://arxiv.org/pdf/1803.08823.pdf,}, 08/10-18.
  
\bibitem{lasso}
  \url{https://www.jstor.org/stable/pdf/2346178.pdf}, 08/10-18.
 
\bibitem{ridge}
  \url{http://math.arizona.edu/~hzhang/math574m/Read/RidgeRegressionBiasedEstimationForNonorthogonalProblems.pdf}, 08/10-18.
  
\bibitem{reg}
  \url{http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf}, 08/10-18.
  

\end{thebibliography}

\end{document}